<?xml version="1.0" encoding="UTF-8"?>
<config-description:config-descriptions
	xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
	xmlns:config-description="https://openhab.org/schemas/config-description/v1.0.0"
	xsi:schemaLocation="https://openhab.org/schemas/config-description/v1.0.0
		https://openhab.org/schemas/config-description-1.0.0.xsd">

	<config-description uri="voice:chatgpthli">
		<parameter-group name="requestParameters">
			<label>Request parameters</label>
			<description>Parameters for creating the ChatGPT completion request body.</description>
		</parameter-group>
		<parameter-group name="serviceParameters">
			<label>HLI service parameters</label>
			<description>Parameters for HLI Service</description>
		</parameter-group>
		<parameter name="chatGPTModel" type="text" required="true" groupName="requestParameters">
			<label>Model</label>
			<description>ID of the model to use.</description>
			<default>gpt-4o</default>
		</parameter>
		<parameter name="temperature" type="decimal" min="0" max="2" groupName="requestParameters">
			<label>Temperature</label>
			<description>What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more
				random, while lower values like 0.2 will make it more focused and deterministic.</description>
			<default>0.5</default>
		</parameter>
		<parameter name="topP" type="decimal" min="0" max="1" groupName="requestParameters">
			<label>TopP</label>
			<description>
				An alternative to sampling with temperature, called nucleus sampling, where the model considers the
				results of the
				tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability
				mass are
				considered.

				We generally recommend altering this or temperature but not both.

			</description>
			<default>1</default>
		</parameter>
		<parameter name="systemMessage" type="text" groupName="requestParameters">
			<label>System Message</label>
			<description>Override the default system message of the assistant.</description>
			<default>You are the manager of the OpenHAB smart home. You know how to manage devices in a smart home or provide
				their current status. You can also answer a question not related to devices in the house. Or, for example, you can
				compose a story upon request.
				I will provide information about the smart home; if necessary, you can perform the
				function; if there is not enough
				information to perform it, then clarify briefly, without listing all the available
				devices and parameters for the
				function. If the question is not related to devices in a smart home, then answer the
				question briefly, maximum 3
				sentences in everyday language.

				The current status of devices is displayed in Available
				Devices.
				The location and purpose of the device is indicated in the device description.
				Use the items_control function
				only for the requested action, not for providing current states.

				Available devices:
			</default>
		</parameter>
		<parameter name="maxTokens" type="integer" groupName="requestParameters">
			<label>Max Tokens</label>
			<description>The maximum number of tokens that can be generated in the chat completion.</description>
			<default>1000</default>
		</parameter>
		<parameter name="keepContext" type="integer" groupName="serviceParameters">
			<label>Keep Context</label>
			<description>How long to store the context in minutes.</description>
			<default>2</default>
		</parameter>
		<parameter name="contextTreshold" type="integer" groupName="serviceParameters">
			<label>Context Treshold</label>
			<description>Limit total tokens included in context.</description>
			<default>10000</default>
		</parameter>
	</config-description>
</config-description:config-descriptions>
